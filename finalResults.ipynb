{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1ac0b5",
   "metadata": {},
   "source": [
    "# Obtencion de Resultados Finales de la Investigacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd316058",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fca9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¬øCu√°l es la diferencia entre una direcci√≥n IP p√∫blica y una privada?', '¬øQu√© es el modelo OSI y cu√°les son sus 7 capas?', '¬øC√≥mo funciona el protocolo TCP y en qu√© se diferencia de UDP?', '¬øQu√© es una m√°scara de subred y para qu√© sirve?', '¬øQu√© es el protocolo ARP y c√≥mo se utiliza en una red local?', '¬øQu√© diferencia hay entre una red LAN, MAN y WAN?', '¬øQu√© es el DNS y c√≥mo resuelve nombres de dominio?', '¬øC√≥mo se detectan y previenen ataques de tipo spoofing en redes?', '¬øC√≥mo se implementa NAT y cu√°l es su funci√≥n en una red dom√©stica?', '¬øCu√°l es la diferencia entre HTTP y HTTPS?', '¬øQu√© es la inteligencia artificial?', '¬øCu√°l es la diferencia entre IA d√©bil y IA fuerte?', '¬øQu√© es un algoritmo de aprendizaje autom√°tico?', '¬øQu√© es una red neuronal artificial?', '¬øQu√© es el procesamiento del lenguaje natural (NLP)?', '¬øQu√© es el aprendizaje por refuerzo?', '¬øQu√© es la visi√≥n por computadora?', '¬øQu√© es el sesgo en los algoritmos de IA y por qu√© es importante?', '¬øQu√© es la explicabilidad en IA y por qu√© es crucial?', '¬øQu√© son los sistemas expertos y c√≥mo funcionan?', '¬øQu√© es un n√∫mero primo?', '¬øQu√© es pi?', '¬øQu√© es una funci√≥n lineal?', '¬øQu√© es la derivada de una funci√≥n?', '¬øQu√© es una integral definida?', '¬øQu√© es una matriz y para qu√© se utiliza?', '¬øQu√© es el teorema de Pit√°goras y en qu√© contexto se aplica?', '¬øPara qu√© se usan los n√∫meros complejos en la vida real?', '¬øQu√© significa que una funci√≥n sea continua y por qu√© eso es importante?', '¬øQu√© es un espacio vectorial y c√≥mo se aplica en problemas del mundo real?', '¬øQu√© es un √°tomo?', '¬øCu√°l es la diferencia entre un elemento y un compuesto?', '¬øQu√© es la tabla peri√≥dica y qu√© informaci√≥n proporciona?', '¬øQu√© es una reacci√≥n qu√≠mica?', '¬øQu√© es la estequiometr√≠a?', '¬øQu√© es un enlace covalente?', '¬øQu√© es la termodin√°mica qu√≠mica?', '¬øQu√© es la cin√©tica qu√≠mica?', '¬øQu√© es la espectroscopia y c√≥mo se utiliza en qu√≠mica?', '¬øQu√© es la qu√≠mica cu√°ntica?', '¬øQu√© es una ley?', '¬øCu√°l es la diferencia entre derecho civil y derecho penal?', '¬øQu√© es un contrato?', '¬øQu√© es la jurisprudencia?', '¬øQu√© es el derecho internacional?', '¬øQu√© es la propiedad intelectual?', '¬øQu√© es el derecho constitucional?', '¬øQu√© es la teor√≠a del derecho?', '¬øQu√© es el derecho comparado?', '¬øQu√© es la filosof√≠a del derecho?', '¬øQu√© es el dinero?', '¬øQu√© es una acci√≥n en el mercado de valores?', '¬øQu√© es un bono?', '¬øQu√© es la inflaci√≥n?', '¬øQu√© es el Producto Interno Bruto (PIB)?', '¬øQu√© es la diversificaci√≥n en inversiones?', '¬øQu√© es el an√°lisis fundamental en finanzas?', '¬øQu√© es la teor√≠a de portafolios?', '¬øQu√© es la econometr√≠a?', '¬øQu√© es la finanza conductual?', '¬øQu√© es una c√©lula?', '¬øCu√°l es la diferencia entre ADN y ARN?', '¬øQu√© es la fotos√≠ntesis?', '¬øQu√© es la evoluci√≥n?', '¬øQu√© es la gen√©tica mendeliana?', '¬øQu√© es la ecolog√≠a?', '¬øQu√© es la biotecnolog√≠a?', '¬øQu√© es la neurobiolog√≠a?', '¬øQu√© es la biolog√≠a molecular?', '¬øQu√© es la bioinform√°tica?', '¬øCu√°l es la diferencia entre multiprocesamiento y multitarea?', '¬øQu√© es un deadlock y c√≥mo se puede evitar?', '¬øEn qu√© consiste la planificaci√≥n de CPU Round Robin y cu√°les son sus ventajas?', '¬øQu√© es la memoria virtual y c√≥mo funciona la paginaci√≥n?', '¬øCu√°l es la diferencia entre un proceso y un hilo (thread)?', '¬øC√≥mo se implementa un sistema de archivos en un sistema operativo?', '¬øQu√© es el modo kernel y modo usuario, y por qu√© es importante esta separaci√≥n?', '¬øQu√© son los sem√°foros y c√≥mo se utilizan para sincronizaci√≥n de procesos?', '¬øQu√© es el sistema de planificaci√≥n por prioridades y qu√© problemas puede generar?', '¬øCu√°l es la funci√≥n del scheduler en un sistema operativo?', '¬øQu√© es un √≥rgano?', '¬øCu√°l es la funci√≥n del coraz√≥n?', '¬øQu√© es una bacteria?', '¬øQu√© es la anatom√≠a humana?', '¬øQu√© es la fisiolog√≠a?', '¬øQu√© es la farmacolog√≠a?', '¬øQu√© es la epidemiolog√≠a?', '¬øQu√© es la cirug√≠a m√≠nimamente invasiva?', '¬øQu√© es la medicina regenerativa?', '¬øQu√© es la neurocirug√≠a?', '¬øQu√© es la fuerza?', '¬øQu√© es la energ√≠a?', '¬øQu√© es la ley de la gravitaci√≥n universal?', '¬øQu√© es la termodin√°mica?', '¬øQu√© es la mec√°nica cu√°ntica?', '¬øQu√© es la relatividad general?', '¬øQu√© es la f√≠sica de part√≠culas?', '¬øQu√© es la √≥ptica cu√°ntica?', '¬øQu√© es la astrof√≠sica?', '¬øQu√© es la teor√≠a de cuerdas?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importacion de preguntas y de respuestas\n",
    "from data.questions import biologia, finanzas, fisica, ia, leyes, matematicas, medicina, quimica, redes, sistemas_operativos \n",
    "from data.biologia import respuestas_biologia\n",
    "from data.finanzas import respuestas_finanzas\n",
    "from data.fisica import respuestas_fisica\n",
    "from data.ia import respuestas_ia\n",
    "from data.leyes import respuestas_leyes\n",
    "from data.matematicas import respuestas_matematicas\n",
    "from data.medicina import respuestas_medicina\n",
    "from data.quimica import respuestas_quimica\n",
    "from data.redes import redes_respuestas\n",
    "from data.sistemas_operativos import respuestas_sistemas_operativos\n",
    "\n",
    "# Importar del sistema original\n",
    "from src.researcher.graph import build_graph  \n",
    "from src.researcher.router import Router\n",
    "from src.researcher.retrieval import Retrieval\n",
    "from src.researcher.judge_graph import crear_sistema_refinamiento\n",
    "\n",
    "# Importacion de Librerias\n",
    "import ollama\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.messages import HumanMessagec:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\tqdm\\auto.py:21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d39944",
   "metadata": {},
   "source": [
    "## Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6943d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:00 - INFO - Use pytorch device_name: cuda\n",
      "2025-09-20 23:29:00 - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "c:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n cargada:\n",
      "   - Modelos: ['mistral', 'llama', 'gemma']\n",
      "   - Temas: 10\n",
      "   - Total preguntas: 100\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "models_names = ['mistral', 'llama', 'gemma']\n",
    "temas = ['biologia', 'finanzas', 'fisica', 'ia', 'leyes', 'matematicas', 'medicina', 'quimica', 'redes', 'sistemas_operativos']\n",
    "listas_preguntas = {\n",
    "    \"biologia\": list(zip(biologia, respuestas_biologia)),\n",
    "    \"finanzas\": list(zip(finanzas, respuestas_finanzas)),\n",
    "    \"fisica\": list(zip(fisica, respuestas_fisica)),\n",
    "    \"ia\": list(zip(ia, respuestas_ia)),\n",
    "    \"leyes\": list(zip(leyes, respuestas_leyes)),\n",
    "    \"matematicas\": list(zip(matematicas, respuestas_matematicas)),\n",
    "    \"medicina\": list(zip(medicina, respuestas_medicina)),\n",
    "    \"quimica\": list(zip(quimica, respuestas_quimica)),\n",
    "    \"redes\": list(zip(redes, redes_respuestas)),\n",
    "    \"sistemas_operativos\": list(zip(sistemas_operativos, respuestas_sistemas_operativos))\n",
    "}\n",
    "\n",
    "model_mapping = {\n",
    "    'mistral': 'mistral:7b',\n",
    "    'llama': 'llama3.1:8b', \n",
    "    'gemma': 'gemma3:4b'\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'mistral': lambda prompt: ollama.generate(model='mistral:7b', prompt=prompt)['response'],\n",
    "    'llama': lambda prompt: ollama.generate(model='llama3.1:8b', prompt=prompt)['response'], \n",
    "    'gemma': lambda prompt: ollama.generate(model='gemma3:4b', prompt=prompt)['response']\n",
    "    \n",
    "}\n",
    "\n",
    "print(f\"Configuraci√≥n cargada:\")\n",
    "print(f\"   - Modelos: {models_names}\")\n",
    "print(f\"   - Temas: {len(temas)}\")\n",
    "print(f\"   - Total preguntas: {sum(len(listas_preguntas[tema]) for tema in temas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18314bc4",
   "metadata": {},
   "source": [
    "## Funcion para el calculo de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7bafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_similitud(respuesta_modelo, respuesta_referencia):\n",
    "    embeddings = model.encode([respuesta_modelo, respuesta_referencia])\n",
    "    similitud_matrix = cosine_similarity(embeddings)\n",
    "    return similitud_matrix[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295c65b",
   "metadata": {},
   "source": [
    "## Funcion para probar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c786fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando modelos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:10 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mistral:  El agua es una sustancia qu√≠mica que tiene la f√≥rmula qu√≠mica H2O, compuesta po...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:16 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llama: El agua es un l√≠quido que ocupa la mayor parte de la Tierra, y es indispensable ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:27 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ gemma: El agua es una sustancia incre√≠blemente importante y compleja, y su definici√≥n p...\n"
     ]
    }
   ],
   "source": [
    "def test_models():\n",
    "    \"\"\"Test r√°pido para verificar que los modelos funcionan\"\"\"\n",
    "    test_prompt = \"¬øQu√© es el agua?\"\n",
    "    print(\"Probando modelos...\")\n",
    "    \n",
    "    for name, model_func in models.items():\n",
    "        try:\n",
    "            response = model_func(test_prompt)\n",
    "            print(f\"‚úÖ {name}: {response[:80]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {name}: Error - {e}\")\n",
    "\n",
    "# Ejecutar test\n",
    "test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1b6df",
   "metadata": {},
   "source": [
    "## Sistema de Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a4dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_sistema_completo_response(query, model_name):\n",
    "    \"\"\"\n",
    "    Llama al sistema completo (con grafo) para obtener una respuesta.\n",
    "    \n",
    "    Args:\n",
    "        query (str): La pregunta a procesar\n",
    "        model_name (str): Nombre del modelo a usar\n",
    "        \n",
    "    Returns:\n",
    "        str: La respuesta del sistema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir el grafo\n",
    "        graph = build_graph()\n",
    "        mapped_model = model_mapping.get(model_name, model_name)\n",
    "        judge_graph = crear_sistema_refinamiento(model_name=mapped_model)\n",
    "\n",
    "        # Estado inicial\n",
    "        state = {\n",
    "            \"messages\": [],\n",
    "            \"investigation\": True,\n",
    "            \"current_query\": \"\",\n",
    "            \"research_plan\": [],\n",
    "            \"retrieval_queries\": [],\n",
    "            \"query_category\": \"\",\n",
    "            \"research_collections\": [],\n",
    "            \"current_step\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"retrieval_results\": {},\n",
    "            \"context_for_generation\": \"\",\n",
    "            \"research_completed\": False,\n",
    "            \"retrieval_obj\": Retrieval(persist_directory=\"./chroma_db\"),\n",
    "            \"router_obj\": Router(model_name=mapped_model),\n",
    "            \"judge_obj\": judge_graph,\n",
    "            \"response_model\": mapped_model\n",
    "        }\n",
    "\n",
    "        state[\"router_obj\"].retriever = state[\"retrieval_obj\"]\n",
    "\n",
    "        # Agregar mensaje y ejecutar\n",
    "        state[\"messages\"].append(HumanMessage(content=query))\n",
    "        state[\"current_query\"] = query\n",
    "\n",
    "        # Ejecutar el grafo\n",
    "        final_state = await graph.ainvoke(state)\n",
    "        \n",
    "        # Extraer la respuesta\n",
    "        if final_state[\"messages\"] and len(final_state[\"messages\"]) > 1:\n",
    "            ai_response = final_state[\"messages\"][-1]\n",
    "            return ai_response.content\n",
    "        else:\n",
    "            return \"No se obtuvo respuesta del sistema completo\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"ERROR en sistema completo: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819635c",
   "metadata": {},
   "source": [
    "## Estructura de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6b571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames creados:\n",
      "   - Distancias: 6 modelos x 200 columnas\n",
      "   - Respuestas: 6 modelos x 100 preguntas\n",
      "   - Total experimentos: 600\n"
     ]
    }
   ],
   "source": [
    "todas_columnas = []\n",
    "for tema in temas:\n",
    "    for i, (pregunta, _) in enumerate(listas_preguntas[tema], 1):\n",
    "        todas_columnas.append(f\"{tema}_p{i}_sim\")\n",
    "        todas_columnas.append(f\"{tema}_p{i}_time\")\n",
    "\n",
    "columnas_respuestas = []\n",
    "for tema in temas:\n",
    "    for i, (pregunta, _) in enumerate(listas_preguntas[tema], 1):\n",
    "        columnas_respuestas.append(f\"{tema}_p{i}\")\n",
    "\n",
    "# Crear √≠ndices para ambos tipos de modelo\n",
    "indices_ollama = [f\"ollama_{model}\" for model in models_names]\n",
    "indices_sistema = [f\"sistema_{model}\" for model in models_names]\n",
    "todos_indices = indices_ollama + indices_sistema\n",
    "\n",
    "# Crear DataFrames\n",
    "df_distancias = pd.DataFrame(index=todos_indices, columns=todas_columnas, dtype=float)\n",
    "df_respuestas = pd.DataFrame(index=todos_indices, columns=columnas_respuestas, dtype=object)\n",
    "\n",
    "print(f\"DataFrames creados:\")\n",
    "print(f\"   - Distancias: {len(todos_indices)} modelos x {len(todas_columnas)} columnas\")\n",
    "print(f\"   - Respuestas: {len(todos_indices)} modelos x {len(columnas_respuestas)} preguntas\")\n",
    "print(f\"   - Total experimentos: {len(todos_indices) * len(columnas_respuestas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b02e3",
   "metadata": {},
   "source": [
    "## Pruebas Comparacion y Obtencion de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1158e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation():\n",
    "    \"\"\"Funci√≥n principal de evaluaci√≥n\"\"\"\n",
    "    \n",
    "    # Iniciar evaluaci√≥n\n",
    "    start_time = time.time()\n",
    "    total_experimentos = len(todos_indices) * sum(len(listas_preguntas[tema]) for tema in temas)\n",
    "    experimento_actual = 0\n",
    "\n",
    "    for tema in temas:\n",
    "        print(f\"\\nProcesando tema: {tema.upper()}\")\n",
    "             \n",
    "        for i, (pregunta, respuesta_ref) in enumerate(listas_preguntas[tema], 1):\n",
    "            col_name = f\"{tema}_p{i}\"\n",
    "            col_sim = f\"{tema}_p{i}_sim\"\n",
    "            col_time = f\"{tema}_p{i}_time\"\n",
    "            print(f\"  ‚ùì P{i}: {pregunta[:60]}...\")\n",
    "                     \n",
    "            # EVALUAR MODELOS OLLAMA DIRECTOS\n",
    "            for modelo_name, model_func in models.items():\n",
    "                experimento_actual += 1\n",
    "                progreso = (experimento_actual / total_experimentos) * 100\n",
    "                index_ollama = f\"ollama_{modelo_name}\"\n",
    "                         \n",
    "                try:\n",
    "                    print(f\"Ollama-{modelo_name}... \")\n",
    "                    tiempo_inicio = time.time()\n",
    "                    \n",
    "                    respuesta_modelo = model_func(pregunta)\n",
    "                    tiempo_respuesta = time.time() - tiempo_inicio\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    df_respuestas.loc[index_ollama, col_name] = respuesta_modelo\n",
    "                    similitud = calcular_similitud(respuesta_modelo, respuesta_ref)\n",
    "                    df_distancias.loc[index_ollama, col_sim] = similitud\n",
    "                    df_distancias.loc[index_ollama, col_time] = tiempo_respuesta\n",
    "                    \n",
    "                    print(f\"Sim: {similitud:.3f}, T: {tiempo_respuesta:.2f}s ({progreso:.1f}%)\")\n",
    "                             \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)[:50]}...\")\n",
    "                    df_respuestas.loc[index_ollama, col_name] = f\"ERROR: {e}\"\n",
    "                    df_distancias.loc[index_ollama, col_sim] = None\n",
    "                    df_distancias.loc[index_ollama, col_time] = None\n",
    "\n",
    "            # EVALUAR SISTEMA COMPLETO\n",
    "            for modelo_name in models_names:\n",
    "                experimento_actual += 1\n",
    "                progreso = (experimento_actual / total_experimentos) * 100\n",
    "                index_sistema = f\"sistema_{modelo_name}\"\n",
    "                         \n",
    "                try:\n",
    "                    print(f\"Sistema-{modelo_name}... \")\n",
    "                    tiempo_inicio = time.time()\n",
    "                    \n",
    "                    respuesta_modelo = await get_sistema_completo_response(pregunta, modelo_name)\n",
    "                    tiempo_respuesta = time.time() - tiempo_inicio\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    df_respuestas.loc[index_sistema, col_name] = respuesta_modelo\n",
    "                    similitud = calcular_similitud(respuesta_modelo, respuesta_ref)\n",
    "                    df_distancias.loc[index_sistema, col_sim] = similitud\n",
    "                    df_distancias.loc[index_sistema, col_time] = tiempo_respuesta\n",
    "                    \n",
    "                    print(f\"Sim: {similitud:.3f}, T: {tiempo_respuesta:.2f}s ({progreso:.1f}%)\")\n",
    "                             \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)[:50]}...\")\n",
    "                    df_respuestas.loc[index_sistema, col_name] = f\"ERROR: {e}\"\n",
    "                    df_distancias.loc[index_sistema, col_sim] = None\n",
    "                    df_distancias.loc[index_sistema, col_time] = None\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTiempo total: {elapsed_time/60:.2f} minutos\")\n",
    "\n",
    "    return df_distancias, df_respuestas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa5ca9",
   "metadata": {},
   "source": [
    "## Ejecutar Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "371028cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando tema: BIOLOGIA\n",
      "  ‚ùì P1: ¬øQu√© es una c√©lula?...\n",
      "Ollama-mistral... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:30 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CUDA error: no kernel image is available for execu...\n",
      "Ollama-llama... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:33 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CUDA error: no kernel image is available for execu...\n",
      "Ollama-gemma... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:41 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "2025-09-20 23:29:41 - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CUDA error: no kernel image is available for execu...\n",
      "Sistema-mistral... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 23:29:45 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-09-20 23:29:45 - INFO - Loading mistral:7b, temperature: 0.1\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resultado_distancias, resultado_respuestas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_evaluation()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Actualizar las variables globales\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_distancias \u001b[38;5;241m=\u001b[39m resultado_distancias\n",
      "Cell \u001b[1;32mIn[7], line 55\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSistema-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m tiempo_inicio \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 55\u001b[0m respuesta_modelo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_sistema_completo_response(pregunta, modelo_name)\n\u001b[0;32m     56\u001b[0m tiempo_respuesta \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tiempo_inicio\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Guardar resultados\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36mget_sistema_completo_response\u001b[1;34m(query, model_name)\u001b[0m\n\u001b[0;32m     42\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_query\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Ejecutar el grafo\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Extraer la respuesta\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1989\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1988\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1989\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[0;32m   1990\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1991\u001b[0m     config,\n\u001b[0;32m   1992\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1993\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1994\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1995\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1996\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1998\u001b[0m ):\n\u001b[0;32m   1999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2000\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1874\u001b[0m, in \u001b[0;36mPregel.astream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1871\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1874\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[0;32m   1875\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1876\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1877\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1878\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1879\u001b[0m     ):\n\u001b[0;32m   1880\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1881\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[0;32m   1882\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langgraph\\pregel\\runner.py:362\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    360\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[0;32m    363\u001b[0m         t,\n\u001b[0;32m    364\u001b[0m         retry_policy,\n\u001b[0;32m    365\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[0;32m    366\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    367\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[0;32m    368\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[0;32m    369\u001b[0m         },\n\u001b[0;32m    370\u001b[0m     )\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langgraph\\pregel\\retry.py:132\u001b[0m, in \u001b[0;36marun_with_retry\u001b[1;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    134\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langgraph\\utils\\runnable.py:445\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    442\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    443\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langgraph\\utils\\runnable.py:231\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n",
      "File \u001b[1;32mc:\\Users\\sceva\\anaconda3\\envs\\rag-Agent\\lib\\site-packages\\langchain_core\\callbacks\\manager.py:231\u001b[0m, in \u001b[0;36mshielded.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mshield(func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resultado_distancias, resultado_respuestas = await run_evaluation()\n",
    "\n",
    "# Actualizar las variables globales\n",
    "df_distancias = resultado_distancias\n",
    "df_respuestas = resultado_respuestas\n",
    "\n",
    "# ===== CELDA 9: GUARDAR RESULTADOS Y ESTAD√çSTICAS =====\n",
    "# Guardar archivos CSV\n",
    "df_distancias.to_csv('resultados_distancia_dual.csv', encoding='utf-8')\n",
    "df_respuestas.to_csv('resultados_respuestas_dual.csv', encoding='utf-8')\n",
    "\n",
    "print(\"Archivos guardados:\")\n",
    "print(\"   - resultados_distancia_dual.csv\")\n",
    "print(\"   - resultados_respuestas_dual.csv\")\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "print(f\"\\nResumen de resultados:\")\n",
    "print(f\"   - Distancias shape: {df_distancias.shape}\")\n",
    "print(f\"   - Respuestas shape: {df_respuestas.shape}\")\n",
    "\n",
    "cols_sim = [col for col in df_distancias.columns if col.endswith('_sim')]\n",
    "cols_time = [col for col in df_distancias.columns if col.endswith('_time')]\n",
    "\n",
    "# An√°lisis por tipo de sistema\n",
    "ollama_rows = [idx for idx in df_distancias.index if idx.startswith('ollama_')]\n",
    "sistema_rows = [idx for idx in df_distancias.index if idx.startswith('sistema_')]\n",
    "\n",
    "ollama_similitudes = df_distancias.loc[ollama_rows, cols_sim].notna().sum().sum()\n",
    "sistema_similitudes = df_distancias.loc[sistema_rows, cols_sim].notna().sum().sum()\n",
    "\n",
    "print(f\"\\nComparaci√≥n de sistemas:\")\n",
    "print(f\"   - Ollama directo: {ollama_similitudes} respuestas v√°lidas\")\n",
    "print(f\"   - Sistema completo: {sistema_similitudes} respuestas v√°lidas\")\n",
    "\n",
    "if ollama_similitudes > 0:\n",
    "    avg_sim_ollama = df_distancias.loc[ollama_rows, cols_sim].mean().mean()\n",
    "    avg_time_ollama = df_distancias.loc[ollama_rows, cols_time].mean().mean()\n",
    "    print(f\"   - Ollama promedio - Sim: {avg_sim_ollama:.3f}, Tiempo: {avg_time_ollama:.2f}s\")\n",
    "\n",
    "if sistema_similitudes > 0:\n",
    "    avg_sim_sistema = df_distancias.loc[sistema_rows, cols_sim].mean().mean()\n",
    "    avg_time_sistema = df_distancias.loc[sistema_rows, cols_time].mean().mean()\n",
    "    print(f\"   - Sistema promedio - Sim: {avg_sim_sistema:.3f}, Tiempo: {avg_time_sistema:.2f}s\")\n",
    "\n",
    "# Mostrar muestra de resultados\n",
    "print(f\"\\nüî¨ Muestra de primeros resultados:\")\n",
    "print(\"Primeras 6 columnas de distancias:\")\n",
    "if len(df_distancias.columns) >= 6:\n",
    "    print(df_distancias.iloc[:, :6])\n",
    "else:\n",
    "    print(df_distancias)\n",
    "\n",
    "print(\"\\nüéâ Evaluaci√≥n completada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423c180",
   "metadata": {},
   "source": [
    "## Analisis Adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8284daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAN√ÅLISIS COMPARATIVO DETALLADO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for tema in temas[:3]:  # Primeros 3 temas como muestra\n",
    "    print(f\"\\nTema: {tema.upper()}\")\n",
    "    \n",
    "    # Obtener columnas de similitud para este tema\n",
    "    cols_tema = [col for col in cols_sim if col.startswith(f\"{tema}_\")]\n",
    "    \n",
    "    if cols_tema:\n",
    "        # Promedios por modelo y sistema\n",
    "        for modelo in models_names:\n",
    "            ollama_idx = f\"ollama_{modelo}\"\n",
    "            sistema_idx = f\"sistema_{modelo}\"\n",
    "            \n",
    "            sim_ollama = df_distancias.loc[ollama_idx, cols_tema].mean()\n",
    "            sim_sistema = df_distancias.loc[sistema_idx, cols_tema].mean()\n",
    "            \n",
    "            time_ollama = df_distancias.loc[ollama_idx, [col.replace('_sim', '_time') for col in cols_tema]].mean()\n",
    "            time_sistema = df_distancias.loc[sistema_idx, [col.replace('_sim', '_time') for col in cols_tema]].mean()\n",
    "            \n",
    "            print(f\"  ü§ñ {modelo}:\")\n",
    "            print(f\"     Ollama:  Sim={sim_ollama:.3f}, Tiempo={time_ollama:.2f}s\")\n",
    "            print(f\"     Sistema: Sim={sim_sistema:.3f}, Tiempo={time_sistema:.2f}s\")\n",
    "            \n",
    "            if not pd.isna(sim_ollama) and not pd.isna(sim_sistema):\n",
    "                diff_sim = sim_sistema - sim_ollama\n",
    "                diff_time = time_sistema - time_ollama\n",
    "                print(f\"     Diferencia: Sim={diff_sim:+.3f}, Tiempo={diff_time:+.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
