{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1ac0b5",
   "metadata": {},
   "source": [
    "# Obtencion de Resultados Finales de la Investigacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd316058",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fca9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¬øCu√°l es la diferencia entre una direcci√≥n IP p√∫blica y una privada?', '¬øQu√© es el modelo OSI y cu√°les son sus 7 capas?', '¬øC√≥mo funciona el protocolo TCP y en qu√© se diferencia de UDP?', '¬øQu√© es una m√°scara de subred y para qu√© sirve?', '¬øQu√© es el protocolo ARP y c√≥mo se utiliza en una red local?', '¬øQu√© diferencia hay entre una red LAN, MAN y WAN?', '¬øQu√© es el DNS y c√≥mo resuelve nombres de dominio?', '¬øC√≥mo se detectan y previenen ataques de tipo spoofing en redes?', '¬øC√≥mo se implementa NAT y cu√°l es su funci√≥n en una red dom√©stica?', '¬øCu√°l es la diferencia entre HTTP y HTTPS?', '¬øQu√© es la inteligencia artificial?', '¬øCu√°l es la diferencia entre IA d√©bil y IA fuerte?', '¬øQu√© es un algoritmo de aprendizaje autom√°tico?', '¬øQu√© es una red neuronal artificial?', '¬øQu√© es el procesamiento del lenguaje natural (NLP)?', '¬øQu√© es el aprendizaje por refuerzo?', '¬øQu√© es la visi√≥n por computadora?', '¬øQu√© es el sesgo en los algoritmos de IA y por qu√© es importante?', '¬øQu√© es la explicabilidad en IA y por qu√© es crucial?', '¬øQu√© son los sistemas expertos y c√≥mo funcionan?', '¬øQu√© es un n√∫mero primo?', '¬øQu√© es pi?', '¬øQu√© es una funci√≥n lineal?', '¬øQu√© es la derivada de una funci√≥n?', '¬øQu√© es una integral definida?', '¬øQu√© es una matriz y para qu√© se utiliza?', '¬øQu√© es el teorema de Pit√°goras y en qu√© contexto se aplica?', '¬øPara qu√© se usan los n√∫meros complejos en la vida real?', '¬øQu√© significa que una funci√≥n sea continua y por qu√© eso es importante?', '¬øQu√© es un espacio vectorial y c√≥mo se aplica en problemas del mundo real?', '¬øQu√© es un √°tomo?', '¬øCu√°l es la diferencia entre un elemento y un compuesto?', '¬øQu√© es la tabla peri√≥dica y qu√© informaci√≥n proporciona?', '¬øQu√© es una reacci√≥n qu√≠mica?', '¬øQu√© es la estequiometr√≠a?', '¬øQu√© es un enlace covalente?', '¬øQu√© es la termodin√°mica qu√≠mica?', '¬øQu√© es la cin√©tica qu√≠mica?', '¬øQu√© es la espectroscopia y c√≥mo se utiliza en qu√≠mica?', '¬øQu√© es la qu√≠mica cu√°ntica?', '¬øQu√© es una ley?', '¬øCu√°l es la diferencia entre derecho civil y derecho penal?', '¬øQu√© es un contrato?', '¬øQu√© es la jurisprudencia?', '¬øQu√© es el derecho internacional?', '¬øQu√© es la propiedad intelectual?', '¬øQu√© es el derecho constitucional?', '¬øQu√© es la teor√≠a del derecho?', '¬øQu√© es el derecho comparado?', '¬øQu√© es la filosof√≠a del derecho?', '¬øQu√© es el dinero?', '¬øQu√© es una acci√≥n en el mercado de valores?', '¬øQu√© es un bono?', '¬øQu√© es la inflaci√≥n?', '¬øQu√© es el Producto Interno Bruto (PIB)?', '¬øQu√© es la diversificaci√≥n en inversiones?', '¬øQu√© es el an√°lisis fundamental en finanzas?', '¬øQu√© es la teor√≠a de portafolios?', '¬øQu√© es la econometr√≠a?', '¬øQu√© es la finanza conductual?', '¬øQu√© es una c√©lula?', '¬øCu√°l es la diferencia entre ADN y ARN?', '¬øQu√© es la fotos√≠ntesis?', '¬øQu√© es la evoluci√≥n?', '¬øQu√© es la gen√©tica mendeliana?', '¬øQu√© es la ecolog√≠a?', '¬øQu√© es la biotecnolog√≠a?', '¬øQu√© es la neurobiolog√≠a?', '¬øQu√© es la biolog√≠a molecular?', '¬øQu√© es la bioinform√°tica?', '¬øCu√°l es la diferencia entre multiprocesamiento y multitarea?', '¬øQu√© es un deadlock y c√≥mo se puede evitar?', '¬øEn qu√© consiste la planificaci√≥n de CPU Round Robin y cu√°les son sus ventajas?', '¬øQu√© es la memoria virtual y c√≥mo funciona la paginaci√≥n?', '¬øCu√°l es la diferencia entre un proceso y un hilo (thread)?', '¬øC√≥mo se implementa un sistema de archivos en un sistema operativo?', '¬øQu√© es el modo kernel y modo usuario, y por qu√© es importante esta separaci√≥n?', '¬øQu√© son los sem√°foros y c√≥mo se utilizan para sincronizaci√≥n de procesos?', '¬øQu√© es el sistema de planificaci√≥n por prioridades y qu√© problemas puede generar?', '¬øCu√°l es la funci√≥n del scheduler en un sistema operativo?', '¬øQu√© es un √≥rgano?', '¬øCu√°l es la funci√≥n del coraz√≥n?', '¬øQu√© es una bacteria?', '¬øQu√© es la anatom√≠a humana?', '¬øQu√© es la fisiolog√≠a?', '¬øQu√© es la farmacolog√≠a?', '¬øQu√© es la epidemiolog√≠a?', '¬øQu√© es la cirug√≠a m√≠nimamente invasiva?', '¬øQu√© es la medicina regenerativa?', '¬øQu√© es la neurocirug√≠a?', '¬øQu√© es la fuerza?', '¬øQu√© es la energ√≠a?', '¬øQu√© es la ley de la gravitaci√≥n universal?', '¬øQu√© es la termodin√°mica?', '¬øQu√© es la mec√°nica cu√°ntica?', '¬øQu√© es la relatividad general?', '¬øQu√© es la f√≠sica de part√≠culas?', '¬øQu√© es la √≥ptica cu√°ntica?', '¬øQu√© es la astrof√≠sica?', '¬øQu√© es la teor√≠a de cuerdas?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dannylimon/miniconda3/envs/rag-Agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importacion de preguntas y de respuestas\n",
    "from data.questions import biologia, finanzas, fisica, ia, leyes, matematicas, medicina, quimica, redes, sistemas_operativos \n",
    "from data.biologia import respuestas_biologia\n",
    "from data.finanzas import respuestas_finanzas\n",
    "from data.fisica import respuestas_fisica\n",
    "from data.ia import respuestas_ia\n",
    "from data.leyes import respuestas_leyes\n",
    "from data.matematicas import respuestas_matematicas\n",
    "from data.medicina import respuestas_medicina\n",
    "from data.quimica import respuestas_quimica\n",
    "from data.redes import redes_respuestas\n",
    "from data.sistemas_operativos import respuestas_sistemas_operativos\n",
    "\n",
    "# Importar del sistema original\n",
    "from src.researcher.graph import build_graph  \n",
    "from src.researcher.router import Router\n",
    "from src.researcher.retrieval import Retrieval\n",
    "from src.researcher.judge_graph import crear_sistema_refinamiento\n",
    "\n",
    "# Importacion de Librerias\n",
    "import torch\n",
    "import ollama\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68afd27",
   "metadata": {},
   "source": [
    "## Prueba de CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6eb6546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando CUDA...\n",
      "CUDA disponible: True\n",
      "N√∫mero de GPUs: 1\n",
      "GPU actual: 0\n",
      "Nombre GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memoria GPU: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Verificando CUDA...\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"N√∫mero de GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU actual: {torch.cuda.current_device()}\")\n",
    "    print(f\"Nombre GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"No hay GPU disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d39944",
   "metadata": {},
   "source": [
    "## Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a286e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 10:53:39 - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device= cuda\n",
      "Configuraci√≥n cargada:\n",
      "   - Modelos: ['mistral']\n",
      "   - Temas: 10\n",
      "   - Total preguntas: 100\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device =', device)\n",
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5', device=device)\n",
    "models_names = ['mistral', 'llama', 'gemma']\n",
    "temas = ['biologia', 'finanzas', 'fisica', 'ia', 'leyes', 'matematicas', 'medicina', 'quimica', 'redes', 'sistemas_operativos']\n",
    "listas_preguntas = {\n",
    "    \"biologia\": list(zip(biologia, respuestas_biologia)),\n",
    "    \"finanzas\": list(zip(finanzas, respuestas_finanzas)),\n",
    "    \"fisica\": list(zip(fisica, respuestas_fisica)),\n",
    "    \"ia\": list(zip(ia, respuestas_ia)),\n",
    "    \"leyes\": list(zip(leyes, respuestas_leyes)),\n",
    "    \"matematicas\": list(zip(matematicas, respuestas_matematicas)),\n",
    "    \"medicina\": list(zip(medicina, respuestas_medicina)),\n",
    "    \"quimica\": list(zip(quimica, respuestas_quimica)),\n",
    "    \"redes\": list(zip(redes, redes_respuestas)),\n",
    "    \"sistemas_operativos\": list(zip(sistemas_operativos, respuestas_sistemas_operativos))\n",
    "}\n",
    "\n",
    "model_mapping = {\n",
    "    'mistral': 'mistral:7b',\n",
    "    'llama': 'llama3.1:8b', \n",
    "    'gemma': 'gemma3:4b'\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'mistral': lambda prompt: ollama.generate(model='mistral:7b', prompt=prompt)['response'],\n",
    "    'llama': lambda prompt: ollama.generate(model='llama3.1:8b', prompt=prompt)['response'], \n",
    "    'gemma': lambda prompt: ollama.generate(model='gemma3:4b', prompt=prompt)['response'] \n",
    "}\n",
    "\n",
    "print(f\"Configuraci√≥n cargada:\")\n",
    "print(f\"   - Modelos: {models_names}\")\n",
    "print(f\"   - Temas: {len(temas)}\")\n",
    "print(f\"   - Total preguntas: {sum(len(listas_preguntas[tema]) for tema in temas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18314bc4",
   "metadata": {},
   "source": [
    "## Funcion para el calculo de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7bafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_similitud(respuesta_modelo, respuesta_referencia):\n",
    "    embeddings = model.encode([respuesta_modelo, respuesta_referencia])\n",
    "    similitud_matrix = cosine_similarity(embeddings)\n",
    "    return similitud_matrix[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295c65b",
   "metadata": {},
   "source": [
    "## Funcion para probar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c786fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando modelos.directamente de ollama..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 11:05:25 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mistral:  El agua (en ingl√©s: water) es una sustancia clara y transparente, inodora y inc...\n"
     ]
    }
   ],
   "source": [
    "def test_models():\n",
    "    \"\"\"Test r√°pido para verificar que los modelos funcionan\"\"\"\n",
    "    test_prompt = \"¬øQu√© es el agua?\"\n",
    "    print(\"Probando modelos directamente de ollama..\")\n",
    "    \n",
    "    for name, model_func in models.items():\n",
    "        try:\n",
    "            response = model_func(test_prompt)\n",
    "            print(f\"{name}: {response[:80]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name}: Error - {e}\")\n",
    "\n",
    "# Ejecutar test\n",
    "test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1b6df",
   "metadata": {},
   "source": [
    "## Sistema de Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a4dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_sistema_completo_response(query, model_name):\n",
    "    \"\"\"\n",
    "    Llama al sistema completo (con grafo) para obtener una respuesta.\n",
    "    \n",
    "    Args:\n",
    "        query (str): La pregunta a procesar\n",
    "        model_name (str): Nombre del modelo a usar\n",
    "        \n",
    "    Returns:\n",
    "        str: La respuesta del sistema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir el grafo\n",
    "        graph = build_graph()\n",
    "        mapped_model = model_mapping.get(model_name, model_name)\n",
    "        judge_graph = crear_sistema_refinamiento(model_name=mapped_model)\n",
    "\n",
    "        # Estado inicial\n",
    "        state = {\n",
    "            \"messages\": [],\n",
    "            \"investigation\": True,\n",
    "            \"current_query\": \"\",\n",
    "            \"research_plan\": [],\n",
    "            \"retrieval_queries\": [],\n",
    "            \"query_category\": \"\",\n",
    "            \"research_collections\": [],\n",
    "            \"current_step\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"retrieval_results\": {},\n",
    "            \"context_for_generation\": \"\",\n",
    "            \"research_completed\": False,\n",
    "            \"retrieval_obj\": Retrieval(persist_directory=\"./chroma_db\"),\n",
    "            \"router_obj\": Router(model_name=mapped_model),\n",
    "            \"judge_obj\": judge_graph,\n",
    "            \"response_model\": mapped_model\n",
    "        }\n",
    "\n",
    "        state[\"router_obj\"].retriever = state[\"retrieval_obj\"]\n",
    "\n",
    "        # Agregar mensaje y ejecutar\n",
    "        state[\"messages\"].append(HumanMessage(content=query))\n",
    "        state[\"current_query\"] = query\n",
    "\n",
    "        # Ejecutar el grafo\n",
    "        final_state = await graph.ainvoke(state)\n",
    "        \n",
    "        # Extraer la respuesta\n",
    "        if final_state[\"messages\"] and len(final_state[\"messages\"]) > 1:\n",
    "            ai_response = final_state[\"messages\"][-1]\n",
    "            return ai_response.content\n",
    "        else:\n",
    "            return \"No se obtuvo respuesta del sistema completo\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"ERROR en sistema completo: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d5396",
   "metadata": {},
   "source": [
    "## Funcion para probar el sistema de graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f04fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 11:05:43 - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando sistema completo con grafos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 11:06:08 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-09-21 11:06:09 - INFO - Loading mistral:7b, temperature: 0.1\n",
      "2025-09-21 11:06:15 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-09-21 11:06:15 - INFO - Consulta clasificada como: general\n",
      "2025-09-21 11:06:15 - INFO - Loading mistral:7b, temperature: 0.1\n",
      "2025-09-21 11:06:18 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sistema-mistral:  El agua (en ingl√©s: water) es una sustancia qu√≠mica que se encuentra en la Tier...\n"
     ]
    }
   ],
   "source": [
    "async def test_sistema_completo():\n",
    "    \"\"\"Test r√°pido para verificar que el sistema completo funciona\"\"\"\n",
    "    test_prompt = \"¬øQu√© es el agua?\"\n",
    "    print(\"Probando sistema completo con grafos...\")\n",
    "    \n",
    "    for model_name in models_names:\n",
    "        try:\n",
    "            response = await get_sistema_completo_response(test_prompt, model_name)\n",
    "            print(f\"‚úÖ Sistema-{model_name}: {response[:80]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Sistema-{model_name}: Error - {e}\")\n",
    "\n",
    "# Ejecutar test\n",
    "await test_sistema_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819635c",
   "metadata": {},
   "source": [
    "## Estructura de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6b571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames creados:\n",
      "   - Distancias: 2 modelos x 200 columnas\n",
      "   - Respuestas: 2 modelos x 100 preguntas\n",
      "   - Total experimentos: 200\n"
     ]
    }
   ],
   "source": [
    "todas_columnas = []\n",
    "for tema in temas:\n",
    "    for i, (pregunta, _) in enumerate(listas_preguntas[tema], 1):\n",
    "        todas_columnas.append(f\"{tema}_p{i}_sim\")\n",
    "        todas_columnas.append(f\"{tema}_p{i}_time\")\n",
    "\n",
    "columnas_respuestas = []\n",
    "for tema in temas:\n",
    "    for i, (pregunta, _) in enumerate(listas_preguntas[tema], 1):\n",
    "        columnas_respuestas.append(f\"{tema}_p{i}\")\n",
    "\n",
    "# Crear √≠ndices para ambos tipos de modelo\n",
    "indices_ollama = [f\"ollama_{model}\" for model in models_names]\n",
    "indices_sistema = [f\"sistema_{model}\" for model in models_names]\n",
    "todos_indices = indices_ollama + indices_sistema\n",
    "\n",
    "# Crear DataFrames\n",
    "df_distancias = pd.DataFrame(index=todos_indices, columns=todas_columnas, dtype=float)\n",
    "df_respuestas = pd.DataFrame(index=todos_indices, columns=columnas_respuestas, dtype=object)\n",
    "\n",
    "print(f\"DataFrames creados:\")\n",
    "print(f\"   - Distancias: {len(todos_indices)} modelos x {len(todas_columnas)} columnas\")\n",
    "print(f\"   - Respuestas: {len(todos_indices)} modelos x {len(columnas_respuestas)} preguntas\")\n",
    "print(f\"   - Total experimentos: {len(todos_indices) * len(columnas_respuestas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b02e3",
   "metadata": {},
   "source": [
    "## Funcion Pruebas Comparacion y Obtencion de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1158e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation():\n",
    "    \"\"\"Funci√≥n principal de evaluaci√≥n\"\"\"\n",
    "    \n",
    "    # Iniciar evaluaci√≥n\n",
    "    start_time = time.time()\n",
    "    total_experimentos = len(todos_indices) * sum(len(listas_preguntas[tema]) for tema in temas)\n",
    "    experimento_actual = 0\n",
    "\n",
    "    for tema in temas:\n",
    "        print(f\"\\nProcesando tema: {tema.upper()}\")\n",
    "             \n",
    "        for i, (pregunta, respuesta_ref) in enumerate(listas_preguntas[tema], 1):\n",
    "            col_name = f\"{tema}_p{i}\"\n",
    "            col_sim = f\"{tema}_p{i}_sim\"\n",
    "            col_time = f\"{tema}_p{i}_time\"\n",
    "            print(f\"P{i}: {pregunta[:60]}...\")\n",
    "                     \n",
    "            # EVALUAR MODELOS OLLAMA DIRECTOS\n",
    "            for modelo_name, model_func in models.items():\n",
    "                experimento_actual += 1\n",
    "                progreso = (experimento_actual / total_experimentos) * 100\n",
    "                index_ollama = f\"ollama_{modelo_name}\"\n",
    "                         \n",
    "                try:\n",
    "                    print(f\"Ollama-{modelo_name}... \")\n",
    "                    tiempo_inicio = time.time()\n",
    "                    \n",
    "                    respuesta_modelo = model_func(pregunta)\n",
    "                    tiempo_respuesta = time.time() - tiempo_inicio\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    df_respuestas.loc[index_ollama, col_name] = respuesta_modelo\n",
    "                    similitud = calcular_similitud(respuesta_modelo, respuesta_ref)\n",
    "                    df_distancias.loc[index_ollama, col_sim] = similitud\n",
    "                    df_distancias.loc[index_ollama, col_time] = tiempo_respuesta\n",
    "                    \n",
    "                    print(f\"Sim: {similitud:.3f}, T: {tiempo_respuesta:.2f}s ({progreso:.1f}%)\")\n",
    "                             \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)[:50]}...\")\n",
    "                    df_respuestas.loc[index_ollama, col_name] = f\"ERROR: {e}\"\n",
    "                    df_distancias.loc[index_ollama, col_sim] = None\n",
    "                    df_distancias.loc[index_ollama, col_time] = None\n",
    "\n",
    "            # EVALUAR SISTEMA COMPLETO\n",
    "            for modelo_name in models_names:\n",
    "                experimento_actual += 1\n",
    "                progreso = (experimento_actual / total_experimentos) * 100\n",
    "                index_sistema = f\"sistema_{modelo_name}\"\n",
    "                         \n",
    "                try:\n",
    "                    print(f\"Sistema-{modelo_name}... \")\n",
    "                    tiempo_inicio = time.time()\n",
    "                    \n",
    "                    respuesta_modelo = await get_sistema_completo_response(pregunta, modelo_name)\n",
    "                    tiempo_respuesta = time.time() - tiempo_inicio\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    df_respuestas.loc[index_sistema, col_name] = respuesta_modelo\n",
    "                    similitud = calcular_similitud(respuesta_modelo, respuesta_ref)\n",
    "                    df_distancias.loc[index_sistema, col_sim] = similitud\n",
    "                    df_distancias.loc[index_sistema, col_time] = tiempo_respuesta\n",
    "                    \n",
    "                    print(f\"Sim: {similitud:.3f}, T: {tiempo_respuesta:.2f}s ({progreso:.1f}%)\")\n",
    "                             \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)[:50]}...\")\n",
    "                    df_respuestas.loc[index_sistema, col_name] = f\"ERROR: {e}\"\n",
    "                    df_distancias.loc[index_sistema, col_sim] = None\n",
    "                    df_distancias.loc[index_sistema, col_time] = None\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTiempo total: {elapsed_time/60:.2f} minutos\")\n",
    "\n",
    "    return df_distancias, df_respuestas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa5ca9",
   "metadata": {},
   "source": [
    "## Ejecutar Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371028cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando tema: BIOLOGIA\n",
      "  ‚ùì P1: ¬øQu√© es una c√©lula?...\n",
      "Ollama-mistral... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 11:09:24 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.06s/it]\n",
      "2025-09-21 11:09:29 - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim: 0.873, T: 60.81s (0.5%)\n",
      "Sistema-mistral... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 11:09:47 - INFO - Loading mistral:7b, temperature: 0.1\n",
      "2025-09-21 11:09:48 - INFO - Backing off send_request(...) for 0.6s (requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "2025-09-21 11:09:52 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-09-21 11:09:52 - INFO - Consulta clasificada como: general\n",
      "2025-09-21 11:09:52 - INFO - Loading mistral:7b, temperature: 0.1\n",
      "2025-09-21 11:09:53 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim: 0.912, T: 66.67s (1.0%)\n",
      "  ‚ùì P2: ¬øCu√°l es la diferencia entre ADN y ARN?...\n",
      "Ollama-mistral... \n"
     ]
    }
   ],
   "source": [
    "resultado_distancias, resultado_respuestas = await run_evaluation()\n",
    "\n",
    "# Actualizar las variables globales\n",
    "df_distancias = resultado_distancias\n",
    "df_respuestas = resultado_respuestas\n",
    "\n",
    "df_distancias.to_csv('resultados_distancia_dual.csv', encoding='utf-8')\n",
    "df_respuestas.to_csv('resultados_respuestas_dual.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef315c44",
   "metadata": {},
   "source": [
    "## Resultados y Estadisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Archivos guardados:\")\n",
    "print(\"   - resultados_distancia_dual.csv\")\n",
    "print(\"   - resultados_respuestas_dual.csv\")\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "print(f\"\\nResumen de resultados:\")\n",
    "print(f\"   - Distancias shape: {df_distancias.shape}\")\n",
    "print(f\"   - Respuestas shape: {df_respuestas.shape}\")\n",
    "\n",
    "cols_sim = [col for col in df_distancias.columns if col.endswith('_sim')]\n",
    "cols_time = [col for col in df_distancias.columns if col.endswith('_time')]\n",
    "\n",
    "# An√°lisis por tipo de sistema\n",
    "ollama_rows = [idx for idx in df_distancias.index if idx.startswith('ollama_')]\n",
    "sistema_rows = [idx for idx in df_distancias.index if idx.startswith('sistema_')]\n",
    "\n",
    "ollama_similitudes = df_distancias.loc[ollama_rows, cols_sim].notna().sum().sum()\n",
    "sistema_similitudes = df_distancias.loc[sistema_rows, cols_sim].notna().sum().sum()\n",
    "\n",
    "print(f\"\\nComparaci√≥n de sistemas:\")\n",
    "print(f\"   - Ollama directo: {ollama_similitudes} respuestas v√°lidas\")\n",
    "print(f\"   - Sistema completo: {sistema_similitudes} respuestas v√°lidas\")\n",
    "\n",
    "if ollama_similitudes > 0:\n",
    "    avg_sim_ollama = df_distancias.loc[ollama_rows, cols_sim].mean().mean()\n",
    "    avg_time_ollama = df_distancias.loc[ollama_rows, cols_time].mean().mean()\n",
    "    print(f\"   - Ollama promedio - Sim: {avg_sim_ollama:.3f}, Tiempo: {avg_time_ollama:.2f}s\")\n",
    "\n",
    "if sistema_similitudes > 0:\n",
    "    avg_sim_sistema = df_distancias.loc[sistema_rows, cols_sim].mean().mean()\n",
    "    avg_time_sistema = df_distancias.loc[sistema_rows, cols_time].mean().mean()\n",
    "    print(f\"   - Sistema promedio - Sim: {avg_sim_sistema:.3f}, Tiempo: {avg_time_sistema:.2f}s\")\n",
    "\n",
    "# Mostrar muestra de resultados\n",
    "print(f\"\\nMuestra de primeros resultados:\")\n",
    "print(\"Primeras 6 columnas de distancias:\")\n",
    "if len(df_distancias.columns) >= 6:\n",
    "    print(df_distancias.iloc[:, :6])\n",
    "else:\n",
    "    print(df_distancias)\n",
    "\n",
    "print(\"\\nEvaluaci√≥n completada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423c180",
   "metadata": {},
   "source": [
    "## Analisis Adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8284daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAN√ÅLISIS COMPARATIVO DETALLADO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for tema in temas[:3]:  # Primeros 3 temas como muestra\n",
    "    print(f\"\\nTema: {tema.upper()}\")\n",
    "    \n",
    "    # Obtener columnas de similitud para este tema\n",
    "    cols_tema = [col for col in cols_sim if col.startswith(f\"{tema}_\")]\n",
    "    \n",
    "    if cols_tema:\n",
    "        # Promedios por modelo y sistema\n",
    "        for modelo in models_names:\n",
    "            ollama_idx = f\"ollama_{modelo}\"\n",
    "            sistema_idx = f\"sistema_{modelo}\"\n",
    "            \n",
    "            sim_ollama = df_distancias.loc[ollama_idx, cols_tema].mean()\n",
    "            sim_sistema = df_distancias.loc[sistema_idx, cols_tema].mean()\n",
    "            \n",
    "            time_ollama = df_distancias.loc[ollama_idx, [col.replace('_sim', '_time') for col in cols_tema]].mean()\n",
    "            time_sistema = df_distancias.loc[sistema_idx, [col.replace('_sim', '_time') for col in cols_tema]].mean()\n",
    "            \n",
    "            print(f\"  ü§ñ {modelo}:\")\n",
    "            print(f\"     Ollama:  Sim={sim_ollama:.3f}, Tiempo={time_ollama:.2f}s\")\n",
    "            print(f\"     Sistema: Sim={sim_sistema:.3f}, Tiempo={time_sistema:.2f}s\")\n",
    "            \n",
    "            if not pd.isna(sim_ollama) and not pd.isna(sim_sistema):\n",
    "                diff_sim = sim_sistema - sim_ollama\n",
    "                diff_time = time_sistema - time_ollama\n",
    "                print(f\"     Diferencia: Sim={diff_sim:+.3f}, Tiempo={diff_time:+.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
